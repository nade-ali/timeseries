# -*- coding: utf-8 -*-
"""
Created on Mon Jul 27 12:48:07 2020

@author: Nade
"""
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import RobustScaler
import numpy as np
import tensorflow as tf
from tensorflow import keras
from matplotlib import rc
from pylab import rcParams

sns.set(style='whitegrid', palette='muted', font_scale=1.5)
rcParams['figure.figsize'] = 22, 10

# =============================================================================
# # ------------------------ DATA IMPORT ------------------------
# =============================================================================
df = pd.read_csv(
  "london_merged.csv",
  parse_dates=['timestamp'], # Date Parsing is used to convert strings into dates.
  index_col="timestamp"
)


# Add columns for hour, day, month, etc
df['hour'] = df.index.hour
df['day_of_month'] = df.index.day
df['day_of_week'] = df.index.dayofweek
df['month'] = df.index.month

# Resample involves summing the data according to a specified time length
df_by_month = df.resample('M').sum()



# =============================================================================
# # ------------------------ DATA PRESENTATION ------------------------
# =============================================================================
# =============================================================================
# fig,(ax5, ax6)= plt.subplots(nrows=2)
# fig.set_size_inches(18, 28)
# 
# fig,(ax1, ax2, ax3, ax4)= plt.subplots(nrows=4)
# fig.set_size_inches(18, 28)
# 
# sns.pointplot(data=df, x='hour', y='cnt', ax=ax1)
# sns.pointplot(data=df, x='hour', y='cnt', hue='is_holiday', ax=ax2)
# sns.pointplot(data=df, x='hour', y='cnt', hue='is_weekend', ax=ax3)
# sns.pointplot(data=df, x='hour', y='cnt', hue='season', ax=ax4)
# sns.lineplot(x=df.index, y="cnt", data=df, ax=ax5)
# sns.lineplot(x=df_by_month.index, y="cnt", data=df_by_month, ax=ax6)
# =============================================================================




# =============================================================================
# # ------------------------ DATASET CREATION ------------------------
# =============================================================================
train_size = int(len(df) * 0.9)
test_size = len(df) - train_size

# df.iloc is used to select data
# Here, df.iloc[a:b, c:d] works by selecting everything from row a to b 
# and column c to d. 
train, test = df.iloc[0:train_size], df.iloc[train_size:len(df)]

# We need to normalize the x variables
# Robustscaler essentially normalizes the data
# The way to use it is to first create a "transformer" variable
# The variable is "fitted", i.e. the median and quartiles are calculated
# Replace the original train & test data with the "transformed" data
# The "transformed" data has been centred and scaled.
f_columns = ['t1', 't2', 'hum', 'wind_speed']
f_transformer = RobustScaler()
f_transformer = f_transformer.fit(train[f_columns].to_numpy())
train.loc[:, f_columns] = f_transformer.transform(
  train[f_columns].to_numpy()
)
test.loc[:, f_columns] = f_transformer.transform(
  test[f_columns].to_numpy()
)

# Normalize the count (y-variable) as well
cnt_transformer = RobustScaler()
cnt_transformer = cnt_transformer.fit(train[['cnt']])
train.loc[:,'cnt'] = cnt_transformer.transform(train[['cnt']])
test.loc[:,'cnt'] = cnt_transformer.transform(test[['cnt']])


# Create a function to generate a time series dataset
# the function works like this: first a "history size" is specified
# The "history size" refers to how long a single "time string" should be
# The number of strings created is len(training set) - history size
# For the y-var, the first "history size" number of values are removed

def create_dataset(X, y, time_steps=1):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        v = X.iloc[i:(i + time_steps)].values
        Xs.append(v)
        ys.append(y.iloc[i + time_steps])
    return np.array(Xs), np.array(ys)

time_steps = 10
X_train, y_train = create_dataset(train, train.cnt, time_steps)
X_test, y_test = create_dataset(test, test.cnt, time_steps)

# =============================================================================
# # ------------------------ MODEL CREATION ------------------------
# =============================================================================

# Sequential refers to a model in which property layers are added
# The first layer that we add is LSTM within the bidirectional layer

# What are units?
"""
The proper intuitive explanation of the 'units' parameter for 
Keras recurrent neural networks is that with units = 1 you get a RNN 
as described in textbooks, and with units = n you get a layer which 
consists of n independent copies of such RNN 
- they'll have identical structure, but as they'll 
be initialized with different weights
"""

# What is the input_shape?
"""
A time series input is (typically) a 3D input which has the 
following dimensions:

Batch dimension - number of samples/rows in a batch
Time dimension - represents the temporal aspect of your data 
(e.g. number of days). Here it is "time_steps".
Input dimension - number of features in a single input 
and a single timestep or basically, the number of variables.
"""
model = keras.Sequential()
model.add(
  keras.layers.Bidirectional(
    keras.layers.LSTM(
      units=128,
      input_shape=(X_train.shape[1], X_train.shape[2])
    )
  )
)

# The Dropout layer randomly sets input units to 0 with a frequency 
# of rate at each step during training time, which helps 
# prevent overfitting.
model.add(keras.layers.Dropout(rate=0.2))

# Dense is the regular deeply connected neural network layer
model.add(keras.layers.Dense(units=1))
model.compile(loss='mean_squared_error', optimizer='adam')

# Epochs refer to how many times the entire training set 
# is to be run through
# Batch size refers to how the training set is to be broken down
# If there are a total of 320 rows in the training data, then 
# The fitting runs 10 times per epoch. 

history = model.fit(
    X_train, y_train,
    epochs=5,
    batch_size=32,
    validation_split=0.1,
    shuffle=False
)

# val_loss is the value of cost function for the 
# cross-validation data and loss is the value of 
# cost function for the training data.
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show()

# Use the model to predict y for the test data
y_pred = model.predict(X_test)

# Inverse transform takes the data back to its original form
y_train_inv = cnt_transformer.inverse_transform(y_train.reshape(1, -1))
y_test_inv = cnt_transformer.inverse_transform(y_test.reshape(1, -1))
y_pred_inv = cnt_transformer.inverse_transform(y_pred)

# Flatten turns the array into a one-dimensional array
# Suitable for plotting
# plt.plot works by taking an x-axis, y-axis and further specifications
plt.plot(np.arange(0, len(y_train)), y_train_inv.flatten(), 'g', label="history")
plt.plot(np.arange(len(y_train), len(y_train) + len(y_test)), y_test_inv.flatten(), marker='.', label="true")
plt.plot(np.arange(len(y_train), len(y_train) + len(y_test)), y_pred_inv.flatten(), 'r', label="prediction")
plt.ylabel('Bike Count')
plt.xlabel('Time Step')
plt.legend()
plt.show();

# Same thing as above but just don't show training data
plt.plot(y_test_inv.flatten(), marker='.', label="true")
plt.plot(y_pred_inv.flatten(), 'r', label="prediction")
plt.ylabel('Bike Count')
plt.xlabel('Time Step')
plt.legend()
plt.show();
